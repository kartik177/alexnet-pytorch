{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "kartik updated.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartik177/alexnet-pytorch/blob/master/kartik_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr_qI43H5D2H"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "from sklearn import datasets"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJZrGErS5D2P"
      },
      "source": [
        "class Neural_Network():\n",
        "    def __init__(self, neurons, Activations): \n",
        "        # arguments: an array \"neurons\" consist of number of neurons for each layer, \n",
        "        # an array \"activations\" consisting of activation functions used for the hidden layers and output layer\n",
        "        self.inputSize = neurons[0] # Number of neurons in input layer\n",
        "        self.outputSize = neurons[-1] # Number of neurons in output layer\n",
        "        self.layers = len(neurons)\n",
        "        self.weights = [] #weights for each layer\n",
        "        self.biases = [] #biases in each layer \n",
        "        self.layer_activations = [] #activations in each layer\n",
        "        for i in range(len(neurons)-1): \n",
        "            self.weights.append(np.random.rand(neurons[i+1],neurons[i])) #weight matrix between layer i and layer i+1\n",
        "            self.biases.append(np.random.rand(neurons[i+1],1))\n",
        "            self.layer_activations.append(Activations[i]) #activations for each layer\n",
        "        \n",
        "            \n",
        "    def sigmoid(self, z): # sigmoid activation function\n",
        "        #Fill in the details to compute and return the sigmoid activation function \n",
        "        return (1.0/(1.0+np.exp(-z)))\n",
        "      \n",
        "    \n",
        "    def sigmoidPrime(self,z): # derivative of sigmoid activation function\n",
        "        #Fill in the details to compute and return the derivative of sigmoid activation function\n",
        "        #print(type(z))\n",
        "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
        "\n",
        "                          \n",
        "    def tanh(self, z): # hyperbolic tan activation function\n",
        "        #Fill in the details to compute and return the tanh activation function    \n",
        "        c = (np.exp(z)- np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
        "        return c\n",
        "    \n",
        "    def tanhPrime(self,x): # derivative of hyperbolic tan activation function\n",
        "        #Fill in the details to compute and return the derivative of tanh activation function\n",
        "        return (1 - (self.tanh(x))**2)\n",
        "                          \n",
        "    def linear(self, z): # Linear activation function\n",
        "        #Fill in the details to compute and return the linear activation function \n",
        "        return z\n",
        "    \n",
        "    def linearPrime(self,x): # derivative of linear activation function\n",
        "        #Fill in the details to compute and return the derivative of activation function    \n",
        "        for y in x:\n",
        "            y = 1\n",
        "        return x\n",
        "\n",
        "    def ReLU(self,z): # ReLU activation function\n",
        "        #Fill in the details to compute and return the ReLU activation function  \n",
        "        \n",
        "        for y in z:\n",
        "            if y<0:\n",
        "                y=0\n",
        "        \n",
        "        return z\n",
        "    \n",
        "    def ReLUPrime(self,z): # derivative of ReLU activation function\n",
        "        #Fill in the details to compute and return the derivative of ReLU activation function\n",
        "        v = []\n",
        "        for y in z:\n",
        "            if y > 0 :\n",
        "                v.append([1])\n",
        "            else:\n",
        "                v.append([0])\n",
        "        f = np.array(v)\n",
        "        \n",
        "        return f\n",
        "    \n",
        "    def forward(self, a): # function of forward pass which will receive input and give the output of final layer\n",
        "        # Write the forward pass using the weights and biases to find the predicted value and return them.\n",
        "        layer_activations_a = [a] #store the input as the input layer activations\n",
        "        layer_dot_prod_z = []\n",
        "        for i, param in enumerate(zip(self.biases, self.weights)):\n",
        "            b, w = param[0], param[1]\n",
        "            \n",
        "            if self.layer_activations[i].lower()=='sigmoid':\n",
        "                z = np.dot(w, a)+b\n",
        "                a = self.sigmoid(z)\n",
        "            elif self.layer_activations[i].lower()=='relu':\n",
        "                z = np.dot(w, a)+b\n",
        "                a = self.ReLU(z)\n",
        "            elif self.layer_activations[i].lower()=='tanh':   \n",
        "                z = np.dot(w, a)+b\n",
        "                a = self.tanh(z)\n",
        "            elif self.layer_activations[i].lower()=='linear':\n",
        "                z = np.dot(w, a)+b\n",
        "                a = self.linear(z) \n",
        "           \n",
        "            layer_dot_prod_z.append(z)    \n",
        "            layer_activations_a.append(a)\n",
        "            \n",
        "        return a, layer_dot_prod_z, layer_activations_a\n",
        "                          \n",
        "            \n",
        "    \n",
        "    def backward(self, x, y, zs, activations): # find the loss and return derivative of loss w.r.t every parameter\n",
        "        # Write the backpropagation algorithm here to find the gradients of weights and biases and return them.\n",
        "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # b, w 3*1\n",
        "        # backward pass\n",
        "        if self.layer_activations[-1].lower()=='sigmoid':\n",
        "            delta = (activations[-1] - y) *  self.sigmoidPrime(zs[-1])\n",
        "        elif self.layer_activations[-1].lower()=='relu':\n",
        "            delta = (activations[-1] - y) *  self.ReLUPrime(zs[-1])\n",
        "        elif self.layer_activations[-1].lower()=='tanh':   \n",
        "            delta = (activations[-1] - y) *  self.tanhPrime(zs[-1])\n",
        "\n",
        "        elif self.layer_activations[-1].lower()=='linear':\n",
        "            delta = (activations[-1] - y) *  self.linearPrime(zs[-1])        \n",
        "        \n",
        "        \n",
        "        \n",
        "    \n",
        "        # fill in the appropriate details for gradients of w and b\n",
        "        \n",
        "        grad_b[-1] = delta\n",
        "        \n",
        "        grad_w[-1] = delta*(activations[-2].transpose())\n",
        "        delta = np.matmul(((self.weights[-1]).transpose()) ,delta)\n",
        "        \n",
        "        for l in range(2, self.layers): # Here l is in backward sense i.e. last l th layer\n",
        "            z = zs[-l]\n",
        "            \n",
        "            if self.layer_activations[-l].lower()=='sigmoid':\n",
        "                prime = self.sigmoidPrime(z)\n",
        "            elif self.layer_activations[-l].lower()=='relu':\n",
        "                \n",
        "                prime = self.ReLUPrime(z)\n",
        "            elif self.layer_activations[-l].lower()=='tanh':   \n",
        "                prime = self.tanhPrime(z)\n",
        "            elif self.layer_activations[-l].lower()=='linear':\n",
        "                prime = self.linearPrime(z)\n",
        "               \n",
        "        \n",
        "            p = np.array(prime[:,0])\n",
        "            #Compute delta, gradients of b and w \n",
        "            \n",
        "           \n",
        "            k =np.matmul((self.weights[-l].transpose()),np.diag(p))\n",
        "            grad_b[-l] = delta\n",
        "            grad_w[-l] = np.matmul((np.diag(p)),delta)\n",
        "            grad_w[-l] = np.matmul(grad_w[-l],(activations[-l-1].transpose()))\n",
        "            delta = np.matmul(k,delta)\n",
        "        \n",
        "        return (grad_b, grad_w)                 \n",
        "\n",
        "    def update_parameters(self, grads, learning_rate): # update the parameters using the gradients\n",
        "        # update weights and biases using the gradients and the learning rate\n",
        "        \n",
        "        grad_b, grad_w = grads[0], grads[1]  \n",
        "       \n",
        "    \n",
        "       \n",
        "        #Implement the update rule for weights  and biases\n",
        "        self.weights = self.weights - np.multiply(learning_rate,grad_w)\n",
        "        self.biases = self.biases - np.multiply(learning_rate,grad_b)\n",
        "        \n",
        "    def loss(self, predicted, actual):\n",
        "        #Implement the loss function\n",
        "        return 0.5*(predicted-actual)**2\n",
        "    \n",
        "    def cross_entropy(self, predicted, actual):\n",
        "        CE = -actual*np.log(predicted)\n",
        "        return sum(CE)     \n",
        "    \n",
        "    \n",
        "    def train(self, X, Y, minibatch=False): # receive the full training data set\n",
        "        \n",
        "        lr = 0.001  # learning rate\n",
        "        epochs = 1000  # number of epochs\n",
        "        loss_list = []\n",
        "        CE_loss_list = []\n",
        "        if minibatch==False:\n",
        "            for e in range(epochs): \n",
        "                losses = []\n",
        "                CE_losses = []\n",
        "                for q in range(len(X)):\n",
        "                    \n",
        "                    train_x = np.resize(X[q],(X[q].shape[0],1)) \n",
        "                    if onehotencoded: \n",
        "                        train_y = np.resize(np.argmax(Y[q]),(1,1)) \n",
        "                    else:\n",
        "                        train_y = np.resize(Y[q],(1,1))\n",
        "                   \n",
        "                    \n",
        "                    out, dot_prod_z, activations_a = self.forward(train_x)\n",
        "                    \n",
        "                    loss = self.loss(out, train_y)\n",
        "                    \n",
        "                    CE_loss = self.cross_entropy(out,train_y)\n",
        "                    grads = self.backward(train_x, train_y, dot_prod_z, activations_a) # find the gradients using backward pass\n",
        "                    self.update_parameters(grads, lr)\n",
        "                 \n",
        "                    losses.append(loss)\n",
        "                    CE_losses.append(CE_loss)\n",
        "                loss_list.append(np.mean(np.array(losses)))\n",
        "                CE_loss_list.append(np.mean(np.array(CE_losses)))\n",
        "                \n",
        "                print(f'Epoch: {e} Loss: {np.mean(np.array(losses))} ')\n",
        "        else:\n",
        "            minibatchsize = 20\n",
        "            create_minibatches(X,Y,minibatchsize)\n",
        "            \n",
        "            for e in range(epochs):\n",
        "                #Complete the training code with minibatches \n",
        "                losses = []\n",
        "                CE_losses = []\n",
        "                minibatch = create_minibatches(X,Y,minibatchsize)\n",
        "                for minibatch in minibatch:\n",
        "                    x_mini,y_mini = minibatch\n",
        "                    x_train = np.resize(x_mini,(minibatchsize, x_mini.shape[1]))\n",
        "                    y_train = []\n",
        "                    for i in range(len(y_mini)):\n",
        "                        if onehotencoded:\n",
        "                            train_y = np.resize(np.argmax(y_mini[i]),(1,1)) \n",
        "                            y_train.append(train_y)\n",
        "                        else:\n",
        "                            y_train.append(y_mini[i])\n",
        "                        \n",
        "                    output, dot_prod_z ,activations_a = self.forward(x_train)\n",
        "                    \n",
        "                    loss = self.loss(output,y_train)\n",
        "                   \n",
        "                    CE_loss = self.cross_entropy(out,y_train)\n",
        "                    grads = self.backward(x_train,y_train, dot_prod_z , activations_a)\n",
        "                    self.update_parameters(grads, lr)\n",
        "                    losses.append(loss)\n",
        "                    CE_losses.append(CE_loss)\n",
        "                loss_list.append(np.mean(np.array(losses)))\n",
        "                CE_loss_list.append(np.mean(np.array(CE_losses)))\n",
        "                \n",
        "                print(f'Epoch: {e} Loss: {np.mean(np.array(losses))} ')\n",
        "        return loss_list\n",
        "        \n",
        "    def predict(self, x):\n",
        "        print (\"Input : \\n\" + str(x))\n",
        "        prediction,_,_ = self.forward(x)\n",
        "        print (\"Output: \\n\" + str(prediction))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKqunRGD5D2U"
      },
      "source": [
        "# a method for creating one hot encoded labels \n",
        "def onehotencoding(Y_data):\n",
        "    \n",
        "    Y = []\n",
        "    for i in range (0,len(Y_data),1):\n",
        "        k = [0]*10\n",
        "        k[Y_data[i]] = 1\n",
        "        Y.append(k) \n",
        "        \n",
        "    return np.array(Y)\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "#a method to create minibatches \n",
        "def create_minibatches(X,Y,minibatchsize):\n",
        "    numbatches = int(np.ceil(len(X)/minibatchsize))\n",
        "    idx = np.arange(len(X))\n",
        "    np.random.shuffle(idx)\n",
        "    X_minibatches = []\n",
        "    Y_minibatches = [] \n",
        "    for i in range(numbatches):\n",
        "        idx_minibatch = idx[i*minibatchsize:min(len(idx),(i+1)*minibatchsize)]\n",
        "        xn = np.take(X,idx_minibatch,axis=0) \n",
        "        yn = np.take(Y,idx_minibatch,axis=0)\n",
        "        X_minibatches.append(xn)\n",
        "        Y_minibatches.append(yn)\n",
        "    return X_minibatches, Y_minibatches\n",
        "\n",
        "def test_create_minibatches():\n",
        "    X = []\n",
        "    Y = []\n",
        "    inputsize = 3\n",
        "    minibatch = True\n",
        "    onehotencoded = False\n",
        "    n_batch = 20\n",
        "    batch_size = 20\n",
        "    for i in range(50):\n",
        "        if(i % 2 == 0):\n",
        "            X.append([np.random.randint(1,10) for i1 in range(inputsize)])\n",
        "            Y.append(1)\n",
        "        else:\n",
        "            X.append([np.random.randint(-10,1) for i1 in range(inputsize)])\n",
        "            Y.append(0)\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    X_mb, Y_mb = create_minibatches(X,Y,6)\n",
        "    print(X_mb, Y_mb)\n",
        "\n",
        "#test_create_minibatches()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZtOtUNz5D2Y"
      },
      "source": [
        "# Generating some training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEn8Kmv-5D2a"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "inputsize = 3\n",
        "minibatch = True\n",
        "onehotencoded = False\n",
        "n_batch = 500\n",
        "batch_size = 20\n",
        "for i in range(500):\n",
        "    if(i % 2 == 0):\n",
        "        X.append([random.randint(1,10) for i1 in range(inputsize)])\n",
        "        Y.append(1)\n",
        "    else:\n",
        "        X.append([random.randint(-10,1) for i1 in range(inputsize)])\n",
        "        Y.append(0)\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "if onehotencoded:\n",
        "    Y = onehotencoding(Y)\n",
        "\n",
        "if minibatch==False:\n",
        "    train_X = X\n",
        "    train_Y = Y\n",
        "else:\n",
        "    train_X = []\n",
        "    train_Y = []\n",
        "    for i in range(n_batch):\n",
        "        xn, yn = create_minibatches(X,Y,batch_size)\n",
        "        train_X.append(xn)\n",
        "        train_Y.append(yn)\n",
        "    train_X = np.concatenate(train_X, axis=0).reshape((n_batch,batch_size,-1))\n",
        "    train_Y = np.concatenate(train_Y, axis=0).reshape((n_batch,batch_size,-1))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LivFALhH5D2g",
        "outputId": "51d01b1c-62e2-411f-a9cd-0fbd76e1e361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#train_X = np.concatenate(train_X, axis=0).reshape((n_batch,batch_size,inputsize))\n",
        "\n",
        "\n",
        "print(train_X.shape)\n",
        "print(train_Y.shape)\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 20, 75)\n",
            "(500, 20, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaEY0n_X5D2n"
      },
      "source": [
        "# Defining the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QARtQ1M5D2o"
      },
      "source": [
        "#D_in is input dimension\n",
        "#H1 is dimension of first hidden layer \n",
        "#H2 is dimension of second hidden layer\n",
        "#D_out is output dimension.\n",
        "D_in, H1, H2, H3, H4, D_out = inputsize, 10, 8,8 ,5 ,1  #You can add more layers if you wish to \n",
        "\n",
        "neurons = [D_in, H1, H2, H3, H4, D_out] # list of number of neurons in the layers sequentially.\n",
        "activation_functions = ['linear','linear','tanh','relu','sigmoid'] #activations in each layer (Note: the input layer does not have any activation)\n",
        "my_neuralnet = Neural_Network(neurons, activation_functions )\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugtq0po85D2s"
      },
      "source": [
        "# Training the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VACb5Chg5D2v",
        "outputId": "b5fca3da-4855-434a-bf1a-65dcf492d891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "\n",
        "loss  = my_neuralnet.train(train_X,train_Y,minibatch=minibatch)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-6c3dc82cf0ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmy_neuralnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-e96ee45e8001>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, minibatch)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mminibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminibatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                     \u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KKApys45D21"
      },
      "source": [
        "# Prediction for a data point after the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "nPgdBQbJ5D23",
        "outputId": "f560028c-dd1f-4623-f37c-1beabcfee378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "my_neuralnet.predict(np.array([8,4,9]).reshape((3,1)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : \n",
            "[[8]\n",
            " [4]\n",
            " [9]]\n",
            "Output: \n",
            "[[0.99998934]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEpvjj4V5D27",
        "outputId": "f2f9dd36-bda1-413d-bc9c-245e2c2e00ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "plt.plot(loss)\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE2DSN_x5D3A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGf_i2fX5D3E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}