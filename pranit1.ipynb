{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "pranit1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartik177/alexnet-pytorch/blob/master/pranit1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzY5gVWEhOng"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "from sklearn import datasets"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVOY1f8whOnm"
      },
      "source": [
        "class Neural_Network():\n",
        "    def __init__(self, neurons, Activations): \n",
        "        # arguments: an array \"neurons\" consist of number of neurons for each layer, \n",
        "        # an array \"activations\" consisting of activation functions used for the hidden layers and output layer\n",
        "        self.inputSize = neurons[0] # Number of neurons in input layer\n",
        "        self.outputSize = neurons[-1] # Number of neurons in output layer\n",
        "        self.layers = len(neurons)\n",
        "        self.weights = [] #weights for each layer\n",
        "        self.biases = [] #biases in each layer \n",
        "        self.layer_activations = [] #activations in each layer\n",
        "        for i in range(len(neurons)-1): \n",
        "            self.weights.append(np.random.rand(neurons[i+1],neurons[i])) #weight matrix between layer i and layer i+1\n",
        "            self.biases.append(np.random.rand(neurons[i+1],1))\n",
        "            self.layer_activations.append(Activations[i]) #activations for each layer\n",
        "        \n",
        "            \n",
        "    def sigmoid(self, z): # sigmoid activation function\n",
        "        #Fill in the details to compute and return the sigmoid activation function \n",
        "        return (1.0/(1.0+np.exp(-z)))\n",
        "      \n",
        "    \n",
        "    def sigmoidPrime(self,z): # derivative of sigmoid activation function\n",
        "        #Fill in the details to compute and return the derivative of sigmoid activation function\n",
        "        #print(type(z))\n",
        "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
        "\n",
        "                          \n",
        "    def tanh(self, z): # hyperbolic tan activation function\n",
        "        #Fill in the details to compute and return the tanh activation function    \n",
        "        c = (np.exp(z)- np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
        "        return c\n",
        "    \n",
        "    def tanhPrime(self,x): # derivative of hyperbolic tan activation function\n",
        "        #Fill in the details to compute and return the derivative of tanh activation function\n",
        "        return (1 - (self.tanh(x))**2)\n",
        "                          \n",
        "    def linear(self, z): # Linear activation function\n",
        "        #Fill in the details to compute and return the linear activation function \n",
        "        return z\n",
        "    \n",
        "    def linearPrime(self,x): # derivative of linear activation function\n",
        "        #Fill in the details to compute and return the derivative of activation function    \n",
        "        for y in x:\n",
        "            y = 1\n",
        "        return x\n",
        "\n",
        "    def ReLU(self,z): # ReLU activation function\n",
        "        #Fill in the details to compute and return the ReLU activation function  \n",
        "        \n",
        "        for y in z:\n",
        "            if y<0:\n",
        "                y=0\n",
        "        \n",
        "        return z\n",
        "    \n",
        "    def ReLUPrime(self,z): # derivative of ReLU activation function\n",
        "        #Fill in the details to compute and return the derivative of ReLU activation function\n",
        "        v = []\n",
        "        for y in z:\n",
        "            if y > 0 :\n",
        "                v.append([1])\n",
        "            else:\n",
        "                v.append([0])\n",
        "        f = np.array(v)\n",
        "        \n",
        "        return f\n",
        "    \n",
        "    def softmax(self, a):\n",
        "        q = []\n",
        "        sum_prob= 0\n",
        "        for j in range (0,len(a),1):\n",
        "            sum_prob = sum_prob + np.exp(a[j])\n",
        "            \n",
        "            \n",
        "        for i in range (0,len(a),1):\n",
        "            q.append(np.exp(a[i])/sum_prob)\n",
        "            \n",
        "            \n",
        "        return np.array(q)\n",
        "    \n",
        "    def forward(self, a): # function of forward pass which will receive input and give the output of final layer\n",
        "        # Write the forward pass using the weights and biases to find the predicted value and return them.\n",
        "        layer_activations_a = [a] #store the input as the input layer activations\n",
        "        layer_dot_prod_z = []\n",
        "        for i, param in enumerate(zip(self.biases, self.weights)):\n",
        "            b, w = param[0], param[1]\n",
        "            \n",
        "            if self.layer_activations[i].lower()=='sigmoid':\n",
        "                z = np.dot(w, a)+b\n",
        "               \n",
        "                a = self.sigmoid(z)\n",
        "                a = self.softmax(a)\n",
        "            elif self.layer_activations[i].lower()=='relu':\n",
        "                z = np.dot(w, a)+b\n",
        "               \n",
        "                a = self.ReLU(z)\n",
        "            elif self.layer_activations[i].lower()=='tanh':   \n",
        "                z = np.dot(w, a)+b\n",
        "               \n",
        "                a = self.tanh(z)\n",
        "            elif self.layer_activations[i].lower()=='linear':\n",
        "                z = np.dot(w, a)+b\n",
        "                a = self.linear(z) \n",
        "            \n",
        "            layer_dot_prod_z.append(z)    \n",
        "            layer_activations_a.append(a)\n",
        "            \n",
        "        \n",
        "        return a, layer_dot_prod_z, layer_activations_a\n",
        "                          \n",
        "            \n",
        "    \n",
        "    def backward(self, x, y, zs, activations): # find the loss and return derivative of loss w.r.t every parameter\n",
        "        # Write the backpropagation algorithm here to find the gradients of weights and biases and return them.\n",
        "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # b, w 3*1\n",
        "        # backward pass\n",
        "        if self.layer_activations[-1].lower()=='sigmoid':\n",
        "            delt = []\n",
        "            \n",
        "            for i in range (0,len(activations[-1]),1):\n",
        "                delt.append(self.softmax(activations[-1][i])* (1- self.softmax(activations[-1][i])) * -y[i]/activations[-1][i])\n",
        "            \n",
        "           \n",
        "        elif self.layer_activations[-1].lower()=='relu':\n",
        "            delta = (activations[-1] - y) *  self.ReLUPrime(zs[-1])\n",
        "        elif self.layer_activations[-1].lower()=='tanh':   \n",
        "            delta = (activations[-1] - y) *  self.tanhPrime(zs[-1])\n",
        "\n",
        "        elif self.layer_activations[-1].lower()=='linear':\n",
        "            delta = (activations[-1] - y) *  self.linearPrime(zs[-1])        \n",
        "        \n",
        "        \n",
        "        \n",
        "    \n",
        "        # fill in the appropriate details for gradients of w and b\n",
        "        delta = np.array(delt)\n",
        "        grad_b[-1] = delta\n",
        "        \n",
        "        grad_w[-1] = delta*(activations[-2].transpose())\n",
        "        delta = np.matmul(((self.weights[-1]).transpose()) ,delta)\n",
        "        \n",
        "        for l in range(2, self.layers): # Here l is in backward sense i.e. last l th layer\n",
        "            z = zs[-l]\n",
        "            \n",
        "            if self.layer_activations[-l].lower()=='sigmoid':\n",
        "                prime = self.sigmoidPrime(z)\n",
        "            elif self.layer_activations[-l].lower()=='relu':\n",
        "                \n",
        "                prime = self.ReLUPrime(z)\n",
        "            elif self.layer_activations[-l].lower()=='tanh':   \n",
        "                prime = self.tanhPrime(z)\n",
        "            elif self.layer_activations[-l].lower()=='linear':\n",
        "                prime = self.linearPrime(z)\n",
        "               \n",
        "        \n",
        "            p = np.array(prime[:,0])\n",
        "            #Compute delta, gradients of b and w \n",
        "            \n",
        "           \n",
        "            k =np.matmul((self.weights[-l].transpose()),np.diag(p))\n",
        "            grad_b[-l] = delta\n",
        "            grad_w[-l] = np.matmul((np.diag(p)),delta)\n",
        "            grad_w[-l] = np.matmul(grad_w[-l],(activations[-l-1].transpose()))\n",
        "            delta = np.matmul(k,delta)\n",
        "        \n",
        "        return (grad_b, grad_w)                 \n",
        "\n",
        "    def update_parameters(self, grads, learning_rate): # update the parameters using the gradients\n",
        "        # update weights and biases using the gradients and the learning rate\n",
        "        \n",
        "        grad_b, grad_w = grads[0], grads[1]  \n",
        "        grad_w = np.array(grad_w)\n",
        "        grad_b = np.array(grad_b)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        for j in range(0,len(grad_w),1):\n",
        "            max_element = grad_w[j].max()\n",
        "            if max_element != 0:\n",
        "                grad_w[j] = grad_w[j]/(max_element)\n",
        "            else:\n",
        "                pass\n",
        "            \n",
        "        for j in range(0,len(grad_b),1):\n",
        "            max_element = grad_b[j].max()\n",
        "            if max_element != 0:\n",
        "                grad_b[j] = grad_b[j]/(max_element)\n",
        "            else:\n",
        "                pass   \n",
        "        \n",
        "       \n",
        "        #Implement the update rule for weights  and biases\n",
        "        self.weights = self.weights - np.multiply(learning_rate,grad_w)\n",
        "        self.biases = self.biases - np.multiply(learning_rate,grad_b)\n",
        "        \n",
        "    def loss(self, predicted, actual):\n",
        "        #Implement the loss function\n",
        "        l= 0.5*(predicted-actual)**2\n",
        "     \n",
        "        return sum(l)\n",
        "    \n",
        "    def cross_entropy(self, predicted, actual):\n",
        "        CE = -actual*np.log(predicted)\n",
        "        return sum(CE)     \n",
        "    \n",
        "    \n",
        "    def train(self, X, Y, minibatch=False): # receive the full training data set\n",
        "        \n",
        "        lr = 0.01        # learning rate\n",
        "        epochs = 25 # number of epochs\n",
        "        loss_list = []\n",
        "        CE_loss_list = []\n",
        "        if minibatch==False:\n",
        "            for e in range(epochs): \n",
        "                losses = []\n",
        "                CE_losses = []\n",
        "                for q in range(len(X)):\n",
        "                    \n",
        "                    train_x = np.resize(X[q],(X[q].shape[0],1)) \n",
        "                    if not onehotencoded: \n",
        "                        train_y = np.resize(Y[q],(1,1)) \n",
        "                    else:\n",
        "                        train_y = np.resize(Y[q],(Y[q].shape[0],1))\n",
        "                    \n",
        "                    \n",
        "                    out, dot_prod_z, activations_a = self.forward(train_x)\n",
        "                    \n",
        "                    loss = self.cross_entropy(out, train_y)\n",
        "                    \n",
        "                    CE_loss = self.cross_entropy(out,train_y)\n",
        "                    grads = self.backward(train_x, train_y, dot_prod_z, activations_a) # find the gradients using backward pass\n",
        "                    self.update_parameters(grads, lr)\n",
        "                 \n",
        "                    losses.append(loss)\n",
        "                    CE_losses.append(CE_loss)\n",
        "                loss_list.append(np.mean(np.array(losses)))\n",
        "                CE_loss_list.append(np.mean(np.array(CE_losses)))\n",
        "                \n",
        "                print(f'Epoch: {e} Loss: {np.mean(np.array(losses))} ')\n",
        "        else:\n",
        "            minibatchsize = 20\n",
        "            create_minibatches(X,Y,minibatchsize)\n",
        "            \n",
        "            for e in range(epochs):\n",
        "                #Complete the training code with minibatches \n",
        "                losses = []\n",
        "                CE_losses = []\n",
        "                minibatch = create_minibatches(X,Y,minibatchsize)\n",
        "                for minibatch in minibatch:\n",
        "                    x_mini,y_mini = minibatch\n",
        "                    x_train = np.resize(x_mini,(minibatchsize, x_mini.shape[1]))\n",
        "                    y_train = []\n",
        "                    for i in range(len(y_mini)):\n",
        "                        if onehotencoded:\n",
        "                            train_y = np.resize(np.argmax(y_mini[i]),(1,1)) \n",
        "                            y_train.append(train_y)\n",
        "                        else:\n",
        "                            y_train.append(y_mini[i])\n",
        "                        \n",
        "                    output, dot_prod_z ,activations_a = self.forward(x_train)\n",
        "                    \n",
        "                    loss = self.cross_entropy(output,y_train)\n",
        "                   \n",
        "                    CE_loss = self.cross_entropy(out,y_train)\n",
        "                    grads = self.backward(x_train,y_train, dot_prod_z , activations_a)\n",
        "                    self.update_parameters(grads, lr)\n",
        "                    losses.append(loss)\n",
        "                    CE_losses.append(CE_loss)\n",
        "                loss_list.append(np.mean(np.array(losses)))\n",
        "                CE_loss_list.append(np.mean(np.array(CE_losses)))\n",
        "                \n",
        "                print(f'Epoch: {e} Loss: {np.mean(np.array(losses))} ')\n",
        "        return loss_list,CE_loss_list\n",
        "        \n",
        "    def predict(self, x):\n",
        "        print (\"Input : \\n\" + str(x))\n",
        "        prediction,_,_ = self.forward(x)\n",
        "        print (\"Output: \\n\" + str(prediction))\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "897BZbHphOnq"
      },
      "source": [
        "# a method for creating one hot encoded labels \n",
        "def onehotencoding(Y_data):\n",
        "    \n",
        "    Y = []\n",
        "    for i in range (0,len(Y_data),1):\n",
        "        k = [0]*10\n",
        "        k[Y_data[i]] = 1\n",
        "        Y.append(k) \n",
        "        \n",
        "    return np.array(Y)\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "#a method to create minibatches \n",
        "def create_minibatches(X,Y,minibatchsize):\n",
        "    numbatches = int(np.ceil(len(X)/minibatchsize))\n",
        "    idx = np.arange(len(X))\n",
        "    np.random.shuffle(idx)\n",
        "    X_minibatches = []\n",
        "    Y_minibatches = [] \n",
        "    for i in range(numbatches):\n",
        "        idx_minibatch = idx[i*minibatchsize:min(len(idx),(i+1)*minibatchsize)]\n",
        "        xn = np.take(X,idx_minibatch,axis=0) \n",
        "        yn = np.take(Y,idx_minibatch,axis=0)\n",
        "        X_minibatches.append(xn)\n",
        "        Y_minibatches.append(yn)\n",
        "    return X_minibatches, Y_minibatches\n",
        "\n",
        "def test_create_minibatches():\n",
        "    X = []\n",
        "    Y = []\n",
        "    inputsize = 3\n",
        "    minibatch = False\n",
        "    onehotencoded = False\n",
        "    n_batch = 20\n",
        "    batch_size = 5\n",
        "    for i in range(50):\n",
        "        if(i % 2 == 0):\n",
        "            X.append([np.random.randint(1,10) for i1 in range(inputsize)])\n",
        "            Y.append(1)\n",
        "        else:\n",
        "            X.append([np.random.randint(-10,1) for i1 in range(inputsize)])\n",
        "            Y.append(0)\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    X_mb, Y_mb = create_minibatches(X,Y,6)\n",
        "    print(X_mb, Y_mb)\n",
        "\n",
        "#test_create_minibatches()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nODdy57NhOnv"
      },
      "source": [
        "# Generating some training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Douq7SR8hOnw"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "inputsize = 64\n",
        "minibatch = False\n",
        "onehotencoded = True\n",
        "n_batch = 20\n",
        "batch_size = 5\n",
        "for i in range(500):\n",
        "    if(i % 2 == 0):\n",
        "        X.append([random.randint(1,10) for i1 in range(inputsize)])\n",
        "        Y.append(1)\n",
        "    else:\n",
        "        X.append([random.randint(-10,1) for i1 in range(inputsize)])\n",
        "        Y.append(0)\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "if onehotencoded:\n",
        "    Y = onehotencoding(Y)\n",
        "\n",
        "if minibatch==False:\n",
        "    train_X = X\n",
        "    train_Y = Y\n",
        "else:\n",
        "    train_X = []\n",
        "    train_Y = []\n",
        "    for i in range(n_batch):\n",
        "        xn, yn = create_minibatch(X,Y,batch_size)\n",
        "        train_X.append(xn)\n",
        "        train_Y.append(yn)\n",
        "    train_X = np.concatenate(train_X, axis=0).reshape((n_batch,batch_size,inputsize))\n",
        "    train_Y = np.concatenate(train_Y, axis=0).reshape((n_batch,batch_size,-1))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5wPpDjGhOn1"
      },
      "source": [
        "data = datasets.load_digits(10,True)\n",
        "train_X= np.array(data[0])\n",
        "train_Y = onehotencoding(data[1])\n",
        "\n",
        "mean = np.sum(train_X,axis = 0)/np.size(train_X,0)\n",
        "sd = np.std(train_X,axis = 0,ddof = 1)\n",
        "\n",
        "for j in range (0,np.size(train_X,1),1):\n",
        "    if sd[j]!=0:\n",
        "        for i in range (0,np.size(train_X,0),1):\n",
        "            train_X[i,j] = (train_X[i,j]- mean[j])/sd[j]\n",
        "    else:\n",
        "        pass\n",
        "  \n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "nd5ND3GnhOn6",
        "outputId": "e4db1bad-576e-40a0-e509-f6c450e8eb2e"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "print(train_X.shape)\n",
        "print(train_Y.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1797, 64)\n",
            "(1797, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHQMrXdYhOoD"
      },
      "source": [
        "# Defining the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iz4hTQbhOoD"
      },
      "source": [
        "#D_in is input dimension\n",
        "#H1 is dimension of first hidden layer \n",
        "#H2 is dimension of second hidden layer\n",
        "#D_out is output dimension.\n",
        "D_in, H1, H2,D_out = inputsize, 256,128, 10 #You can add more layers if you wish to \n",
        "\n",
        "neurons = [D_in, H1, H2, D_out] # list of number of neurons in the layers sequentially.\n",
        "activation_functions = ['sigmoid','sigmoid','sigmoid'] #activations in each layer (Note: the input layer does not have any activation)\n",
        "my_neuralnet = Neural_Network(neurons, activation_functions )\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx0cTsaVhOoJ"
      },
      "source": [
        "# Training the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "--NHqMmhhOoJ",
        "outputId": "3389c786-7ff8-4581-ea63-d480524968fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "\n",
        "loss,CE_loss = my_neuralnet.train(train_X,train_Y,minibatch=minibatch)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Loss: 2.3039369755266805 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-040eb657cc1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCE_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_neuralnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-b4ef892b8136>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, minibatch)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot_prod_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b4ef892b8136>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_activations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWuSSlBkhOoN"
      },
      "source": [
        "# Prediction for a data point after the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BiTadg4hhOoO"
      },
      "source": [
        "my_neuralnet.predict(train_X[0].reshape(64,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTNgAHzXhOoT"
      },
      "source": [
        "plt.plot(loss)\n",
        "plt.plot(CE_loss)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAQGCHJjhOoW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnBqbN-xhOoa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5WjxclhhOoe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}