{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alexnet",
      "provenance": [],
      "authorship_tag": "ABX9TyOTqcK1i+ymcDEmF34uoSiT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartik177/alexnet-pytorch/blob/master/alexnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUqkFKHamDKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "# define pytorch device - useful for device-agnostic execution\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define model parameters\n",
        "NUM_EPOCHS = 90  # original paper\n",
        "BATCH_SIZE = 128\n",
        "MOMENTUM = 0.9\n",
        "LR_DECAY = 0.0005\n",
        "LR_INIT = 0.01\n",
        "IMAGE_DIM = 227  # pixels\n",
        "NUM_CLASSES = 1000  # 1000 classes for imagenet 2012 dataset\n",
        "DEVICE_IDS = [0, 1, 2, 3]  # GPUs to use\n",
        "# modify this to point to your data directory\n",
        "INPUT_ROOT_DIR = 'alexnet_data_in'\n",
        "TRAIN_IMG_DIR = 'alexnet_data_in/imagenet'\n",
        "OUTPUT_DIR = 'alexnet_data_out'\n",
        "LOG_DIR = OUTPUT_DIR + '/tblogs'  # tensorboard logs\n",
        "CHECKPOINT_DIR = OUTPUT_DIR + '/models'  # model checkpoints\n",
        "\n",
        "# make checkpoint path directory\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network model consisting of layers propsed by AlexNet paper.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=1000):\n",
        "        \"\"\"\n",
        "        Define and allocate layers for this neural net.\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): number of classes to predict with this model\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # input size should be : (b x 3 x 227 x 227)\n",
        "        # The image in the original paper states that width and height are 224 pixels, but\n",
        "        # the dimensions after first convolution layer do not lead to 55 x 55.\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  # (b x 96 x 55 x 55)\n",
        "            nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  # section 3.3\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 96 x 27 x 27)\n",
        "            nn.Conv2d(96, 256, 5, padding=2),  # (b x 256 x 27 x 27)\n",
        "            nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 13 x 13)\n",
        "            nn.Conv2d(256, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 256, 3, padding=1),  # (b x 256 x 13 x 13)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 6 x 6)\n",
        "        )\n",
        "        # classifier is just a name for linear layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5, inplace=True),\n",
        "            nn.Linear(in_features=(256 * 6 * 6), out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5, inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "        self.init_bias()  # initialize bias\n",
        "\n",
        "    def init_bias(self):\n",
        "        for layer in self.net:\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "                nn.init.constant_(layer.bias, 0)\n",
        "        # original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers\n",
        "        nn.init.constant_(self.net[4].bias, 1)\n",
        "        nn.init.constant_(self.net[10].bias, 1)\n",
        "        nn.init.constant_(self.net[12].bias, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Pass the input through the net.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): input tensor\n",
        "\n",
        "        Returns:\n",
        "            output (Tensor): output tensor\n",
        "        \"\"\"\n",
        "        x = self.net(x)\n",
        "        x = x.view(-1, 256 * 6 * 6)  # reduce the dimensions for linear layer input\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # print the seed value\n",
        "    seed = torch.initial_seed()\n",
        "    print('Used seed : {}'.format(seed))\n",
        "\n",
        "    tbwriter = SummaryWriter(log_dir=LOG_DIR)\n",
        "    print('TensorboardX summary writer created')\n",
        "\n",
        "    # create model\n",
        "    alexnet = AlexNet(num_classes=NUM_CLASSES).to(device)\n",
        "    # train on multiple GPUs\n",
        "    alexnet = torch.nn.parallel.DataParallel(alexnet, device_ids=DEVICE_IDS)\n",
        "    print(alexnet)\n",
        "    print('AlexNet created')\n",
        "\n",
        "    # create dataset and data loader\n",
        "    dataset = datasets.ImageFolder(TRAIN_IMG_DIR, transforms.Compose([\n",
        "        # transforms.RandomResizedCrop(IMAGE_DIM, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "        transforms.CenterCrop(IMAGE_DIM),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]))\n",
        "    print('Dataset created')\n",
        "    dataloader = data.DataLoader(\n",
        "        dataset,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=8,\n",
        "        drop_last=True,\n",
        "        batch_size=BATCH_SIZE)\n",
        "    print('Dataloader created')\n",
        "\n",
        "    # create optimizer\n",
        "    # the one that WORKS\n",
        "    optimizer = optim.Adam(params=alexnet.parameters(), lr=0.0001)\n",
        "    ### BELOW is the setting proposed by the original paper - which doesn't train....\n",
        "    # optimizer = optim.SGD(\n",
        "    #     params=alexnet.parameters(),\n",
        "    #     lr=LR_INIT,\n",
        "    #     momentum=MOMENTUM,\n",
        "    #     weight_decay=LR_DECAY)\n",
        "    print('Optimizer created')\n",
        "\n",
        "    # multiply LR by 1 / 10 after every 30 epochs\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "    print('LR Scheduler created')\n",
        "\n",
        "    # start training!!\n",
        "    print('Starting training...')\n",
        "    total_steps = 1\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        lr_scheduler.step()\n",
        "        for imgs, classes in dataloader:\n",
        "            imgs, classes = imgs.to(device), classes.to(device)\n",
        "\n",
        "            # calculate the loss\n",
        "            output = alexnet(imgs)\n",
        "            loss = F.cross_entropy(output, classes)\n",
        "\n",
        "            # update the parameters\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # log the information and add to tensorboard\n",
        "            if total_steps % 10 == 0:\n",
        "                with torch.no_grad():\n",
        "                    _, preds = torch.max(output, 1)\n",
        "                    accuracy = torch.sum(preds == classes)\n",
        "\n",
        "                    print('Epoch: {} \\tStep: {} \\tLoss: {:.4f} \\tAcc: {}'\n",
        "                        .format(epoch + 1, total_steps, loss.item(), accuracy.item()))\n",
        "                    tbwriter.add_scalar('loss', loss.item(), total_steps)\n",
        "                    tbwriter.add_scalar('accuracy', accuracy.item(), total_steps)\n",
        "\n",
        "            # print out gradient values and parameter average values\n",
        "            if total_steps % 100 == 0:\n",
        "                with torch.no_grad():\n",
        "                    # print and save the grad of the parameters\n",
        "                    # also print and save parameter values\n",
        "                    print('*' * 10)\n",
        "                    for name, parameter in alexnet.named_parameters():\n",
        "                        if parameter.grad is not None:\n",
        "                            avg_grad = torch.mean(parameter.grad)\n",
        "                            print('\\t{} - grad_avg: {}'.format(name, avg_grad))\n",
        "                            tbwriter.add_scalar('grad_avg/{}'.format(name), avg_grad.item(), total_steps)\n",
        "                            tbwriter.add_histogram('grad/{}'.format(name),\n",
        "                                    parameter.grad.cpu().numpy(), total_steps)\n",
        "                        if parameter.data is not None:\n",
        "                            avg_weight = torch.mean(parameter.data)\n",
        "                            print('\\t{} - param_avg: {}'.format(name, avg_weight))\n",
        "                            tbwriter.add_histogram('weight/{}'.format(name),\n",
        "                                    parameter.data.cpu().numpy(), total_steps)\n",
        "                            tbwriter.add_scalar('weight_avg/{}'.format(name), avg_weight.item(), total_steps)\n",
        "\n",
        "            total_steps += 1\n",
        "\n",
        "        # save checkpoints\n",
        "        checkpoint_path = os.path.join(CHECKPOINT_DIR, 'alexnet_states_e{}.pkl'.format(epoch + 1))\n",
        "        state = {\n",
        "            'epoch': epoch,\n",
        "            'total_steps': total_steps,\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'model': alexnet.state_dict(),\n",
        "            'seed': seed,\n",
        "        }\n",
        "        torch.save(state, checkpoint_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}